# Assignment 3
Simon Wicky and Samuel Chassot

## Data races

#### Two threads trying to insert a node in the same place
The list contains the elements 5 and 10, P0 is trying to insert element 7, and P1 want to insert 8. Both processors have created their nodes and the state of variables is as follows :

* **P0** : 	previous =  node(5), current = node(10)
* **P1** : 	previous =  node(5), current = node(10)

At some point the following sequence of events happens :

1. **P0** : new_node->next = current; 
2. **P1** : new_node->next = current;
3. **P0** : previous->next = new_node;
4. **P1** : previous->next = new_node;

The effect is that the list now only contains 5,8 and 10. A race between operations **3** and **4** resulted in a unexpected behavior of the list.

### One thread trying to add, one thread trying to delete

The list contains the elements 5, 8 and 10. P0 is executing insert(7) and P1 is executing delete(8).
Both processors have found the place where they will perform their operations, and the state of variables is as follows :

* **P0** : previous = node(5), current = node(8)
* **P1** : previous = node(5), current = node(8)

At some point the following sequence of events happens :

1. **P0** : new_node->next = current;
2. **P1** : previous->next = current->next;
3. **P0** : previous->next = new_node;

The effect is that the list now contains 5, 7, 8 and 10. Because of the race between op **2** and **3**, the list can behave as if the delete never occured. The *to_remove* flag on the node(8) is still set, and when it will effectively deleted, this can lead the unauthorized memory access.

### Two threads trying to delete two contiguous elements

The list contains the elements 5, 7, 8 and 10. P0 is executing delete(7) and P1 is executing delete(8).
Both processors have found the place where they will perform their operations, and the state of variables is as follows :

* **P0** : previous = node(5), current = node(7)
* **P1** : previous = node(7), current = node(8)

At some point the following sequence of events happens :

1. **P0** : previous->next = current->next;
2. **P1** : previous->next = current->next;

The effect is that the list now contains 5, 8 and 10.
Because **P1** wasn't aware that node(7) was deleted, it couldn't create the right link to delete 8. As before, an unauthorized memory access can occur, since 8 is still linked, but the *to_remove* flag is set.

### Search operations

As the search operations doens't modify anything in the list, no races can occur. As operations in two threads don't have any happens-before relationship, the behavior of search during an addition or a deletion is undefined. There is no way to ensure that the added/deleted element will be included/excluded from the search.

## Single lock
To simplify and shorten the code snippet, only relevant parts are mentioned. Comments indicate where code lines were omitted.

```C
omp_lock_t lock;
omp_init_lock(&lock);

int insert(node_t* head, int val) {
	omp_set_lock(&lock);
	//omitted code

	if (current && current->val == val) {
		omp_unset_lock(&lock);
        return -1;
    }
    
    //omitted code
    omp_unset_lock(&lock);
    return 0;
} 

int delete(node_t* head, int val) {
	omp_set_lock(&lock);
   	node_t *previous, *current;
	if (head->next == NULL) {
		omp_unset_lock(&lock);
      	return -1;
    }
	
	//omitted code
	if (current->val == val) {
		//omitted code
		omp_unset_lock(&lock);
		return val;
	}
	
	//omitted code
   
   omp_unset_lock(&lock); 
	return -1;

}
```

## Performance bottleneck of single lock

With a single lock, every operation on the list is done in a sequenstial way, and thus the benefits of parallelism is lost, even though we could add elements in two different places without introducing races.

## Item lock
A list item now contains a lock, which is initialized when the node is created.
During the search, a thread maintains two locks, one on *previous* and one on *current*. If it is not the right place to insert or delete, the lock of *previous* is unset, *current* becomes *previous*, and the lock of the new *current* is set. This way, no thread can overtake another thread, keeping the integrity of the data structure. To avoid deadlock, the locks are always taken in the same order, i.e *previous* first, and then *current*. Both locks are released when exiting the function.
 
```C
typedef struct node {
    int val;
    struct node *next;
    int to_remove;
    omp_lock_t lock;
} node_t;

int insert(node_t *head, int val) {
    node_t *previous, *current = NULL;
    current = head;
    omp_set_lock(&current->lock);

    while (current && current->val < val) {
        if (previous){
            omp_unset_lock(&previous->lock);
        }
        previous = current;
        current  = current->next;
        omp_set_lock(&current->lock);
    }

    if (current && current->val == val) { // This value already exists!
        omp_unset_lock(&previous->lock);
        omp_unset_lock(&current->lock);
        return -1;
    }
    
    // Here is the right position to insert the new node.
    node_t *new_node;
    new_node = malloc(sizeof(node_t));
    new_node->val = val;
    new_node->next = current;
    new_node->to_remove = 0;
    omp_init_lock(&new_node->lock);

    previous->next = new_node;
    omp_unset_lock(&previous->lock);
    omp_unset_lock(&current->lock);
    return 0;
}

int delete(node_t *head, int val) {
    node_t *previous, *current;

    if (head->next == NULL) { // The list is empty.
        return -1;
    }

    previous = head;
    current = head->next;
    omp_set_lock(&previous->lock);
    omp_set_lock(&current->lock);
    while (current) {
        if (current->val == val) {
            previous->next = current->next;
            current->to_remove = 1; // Another system component will free this node later
            omp_unset_lock(&previous->lock);
            omp_unset_lock(&current->lock);
            return val;
        }
        omp_unset_lock(&previous->lock);
        previous = current;
        current  = current->next;
        omp_set_lock(&current->lock);

    }
    omp_unset_lock(&previous->lock);
    omp_unset_lock(&current->lock);
    return -1;
}
```
## Performance bottleneck of item lock
The implementation of item lock is faster than the single lock, as two threads can insert or delete at the same time, if they are not in the same place of the list. 
A major set back still exists though. As a thread can not overtake another one, this thread can be blocked if the operation he has to do happens after the one already in position. For exemple, if the list consists of number from 1 to 100, Thread 1 with operations delete(99) will be blocked behind Thread 0 performing delete(10), even when they could delete both elements simultaneously

## Lock free linked-list
As for the previous part, list item contains a lock that is initialized when the element is added. In this version, the thread is searching the place to add or delete without locking anything. When it arrives at the right place, it locks *previous* and *current*. For the *insert* case, it checks if *previous->next* indeed equals *current* and if *previous* is not deleted (*to_remove*). If both checks pass that means it is actually a the right place and it performs the add and then releases both elements. If one condition is false, it releases both nodes and recall insert. Cooncerning *delete* function, if *current* equals *previous->next* like for *insert* and if it is ok, it removes and releases both nodes. If the condition fails, it just releases nodes and recall *delete*.

```C
typedef struct node {
    int val;
    struct node *next;
    int to_remove;
    omp_lock_t lock;
} node_t;

int insert(node_t *head, int val) {
    node_t *previous, *current;
    current = head;

    while (current && current->val < val) {
        previous = current;
        current  = current->next;
    }

    if (current && current->val == val) { // This value already exists!
        return -1;
    }
    
    // Here is the right position to insert the new node.
    
    omp_set_lock(&current->lock);
    omp_set_lock(&previous->lock);
    
    if(previous->to_remove || previous->next != current){
        omp_unset_lock(&current->lock);
        omp_unset_lock(&previous->lock);
        return insert(head, val);
    }

    node_t *new_node;
    new_node = malloc(sizeof(node_t));
    new_node->val = val;
    new_node->next = current;
    new_node->to_remove = 0
    omp_init_lock(new_node->lock)

    previous->next = new_node;

    omp_unset_lock(&current->lock);
    omp_unset_lock(&previous->lock);
    return 0;
}

int delete(node_t *head, int val) {
    node_t *previous, *current;

    if (head->next == NULL) { // The list is empty.
        return -1;
    }

    previous = head;
    current = head->next;
    while (current) {
        if (current->val == val) {
            omp_set_lock(&current->lock);
            omp_set_lock(&previous->lock);
            if(previous->next != current || current->to_remove){
                omp_unset_lock(&current->lock);
                omp_unset_lock(&previous->lock);
                return delete(head, val);
            }
            previous->next = current->next;
            current->to_remove = 1; // Another system component will free this node later
            omp_unset_lock(&current->lock);
            omp_unset_lock(&previous->lock);
            return val;
        }

        previous = current;
        current  = current->next;
    }
    return -1;
}


int search(node_t *head, int val) {
    node_t *current = head->next;

    while (current) {
        if (current->val == val) {
            return 0;
        }
        current  = current->next;
    }

    return -1;
}

```

Data races :

* **Two threads try to write at the same place at the same time**
    
    Only the first thread will be able to lock. When it will unlock nodes, the next thread will be able to lock them. If the first thread, modified something, the second thread will have a false to the condition so it will recall the function and do it again.

* **One thread writes, one thread deletes** and **Two threads delete**

    Same explanation as for two writes.


## Is this solving question 5's problem ?
Because the threads do not lock nodes when do the list traversal, they can overtake each other. To see that, let's take the list [1, 6, 10, 12] and say that thread 1 wants to add element 2 and thread 2 wants to delete element 12. T1 goes through the list and finds that it will insert it between 1 and 6. It locks these two nodes. T2 goes through the list and can pass over 1 and 6 because it does not want to lock them (this even if they are already locked by T1). This lets different threads work at different places of the list freely.

## Situations where item locks is better
It is better to lock element like done in question 4 where threads will do changes often at the same place. With the non-locking traversal version, threads will often have to retraverse from the begining. Instead with the locking version, they will have to wait behind the one that has the lock but when it has finished the next can immediatly perfom its action without having to traverse everything again after realizing that there was some changes done.
